<html>
<head>
<style type="text/css">
<!--
a:link {  color: #0074CA; text-decoration: none}
a[href]:hover {  color: #0074CA; text-decoration: none; background-color: #D3DBFE}
a:visited {  color: #0084E6; text-decoration: none}
.justification { font-family: Arial, Helvetica, sans-serif; font-size: 7pt; letter-spacing: normal; text-align: justify; word-spacing: normal ; font-style: normal; font-weight: normal; font-variant: normal; white-space: normal}
-->
</style>
<title>Ontology Alignment Evaluation 2007::benchmarks results</title>
</head>
<body>

<!--a href=".."><img width="20%" border="0" src="../../../oaei.jpg"
		  align="right" /></a>

<table border="1"><tr><td bgcolor="orange"><b>Note:</b>The first results provided at the Ontology matching workshop
  and reproduced in its proceedings on CD-Rom were incorrect as far as
  line 2xx of Table 1 and Table 2 were concerned.</td></tr></table-->

<h1>Benchmark results</h1>

<p>The goal of the benchmark tests is to provide a stable and detailed
  picture of each algorithm. For that purpose, the algorithms are run
  on systematically generated test cases.</p>

<p>Since the goal of these tests is to offer some kind of permanent benchmarks to be used by many, the test is an extension of the 2004 EON Ontology Alignment Contest, whose numbering it (almost) fully preserves.
This year, no modification has been made since the last year benchmark suite.
</p>

<p>13 systems participated in the benchmark track of this year's campaign.
</p>
<p>The evaluation has been performed on the files provided by the
  participants (available <a href="EXTRACTED.zip">here</a>) which have
  been processed by the
  following <a href="process.sh">script</a>).</p>


<h2>Precision and recall results</h2>

<p>Table&nbsp;1 provides the consolidated results, by groups of tests.
We display the results of participants as well as those given by some simple edit distance algorithm on labels (edna).
The computed values are real precision and recall and not an average of precision and recall.</p>

<p>These results show already that three systems are relatively ahead (ASMOV, Lily and RiMOM) with three close followers (Falcon, Prior+ and OLA2). No system had strictly lower performance than edna.</p>
<p>
Each algorithm has its best score with the 1xx test series.
There is no particular order between the two other series.
Again, it is more interesting to look at the 2xx series structure to distinguish the strengths of algorithms.
</p>
<p>
The results have also been compared with the three measures proposed in [Ehrig&nbsp;2005] (symmetric, effort-based and oriented). These are generalisation
of precision and recall in order to better discriminate systems that slightly miss the target from those which are grossly wrong.
The three measures provide the same results, so they have been displayed only once in Table&nbsp;1 under the label "Ext". This is not really surprising given the proximity of these
measures. As expected, they only improve over traditional precision and recall.
Again, the new measures do not dramatically change the evaluation of the participating systems (all score are improved and the six leading systems are closer to each others). This indicates that the not immediately best systems (Falcon, OLA2) could certainly easily be corrected to reach the level of the best ones (RiMOM in particular).
Since last year the implementation of the precision and recall evaluator has changed.
As a consequence, a number of results which would have been rejected last year,
and then corrected by the participants, were accepted this year.
As a consequence, now, the extended precision and recall reject them: this concerns the systems marked with "Error".
</p>

<p>

<table border='2' frame='sides' rules='groups'>
<colgroup align='center' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<thead valign='top'><tr><th>algo</th>
<th colspan='2'>refalign</th>
<th colspan='2'>edna</th>
<th colspan='2'>ASMOV</th>
<th colspan='2'>DSSim</th>
<th colspan='2'>falcon</th>
<th colspan='2'>lily</th>
<th colspan='2'>ola</th>
<th colspan='2'>OntoDNA</th>
<th colspan='2'>OWL-CM</th>
<th colspan='2'>priorplus</th>
<th colspan='2'>RiMOM</th>
<th colspan='2'>sambo</th>
<th colspan='2'>SEMA</th>
<th colspan='2'>TaxoMap</th>
<th colspan='2'>xsom</th>
</tr></thead><tbody><tr><td>test</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
<td>Prec.</td>
<td>Rec.</td>
</tr></tbody><tbody>


<tr bgcolor="lightblue"><td>1xx</td><td>1.00</td>
<td>1.00</td>
<td>0.96</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>0.94</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>0.98</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>0.34</td>
<td>0.99</td>
<td>0.99</td>
</tr>

<tr><td>2xx</td><td>1.00</td>
<td>1.00</td>
<td>0.40</td>
<td>0.55</td>
<td>0.95</td>
<td>0.90</td>
<td>0.99</td>
<td>0.60</td>
<td>0.92</td>
<td>0.85</td>
<td>0.97</td>
<td>0.89</td>
<td>0.91</td>
<td>0.86</td>
<td>0.80</td>
<td>0.43</td>
<td>0.82</td>
<td>0.51</td>
<td>0.92</td>
<td>0.79</td>
<td>0.97</td>
<td>0.86</td>
<td>0.98</td>
<td>0.51</td>
<td>0.92</td>
<td>0.72</td>
<td>0.91</td>
<td>0.19</td>
<td>0.73</td>
<td>0.67</td>
</tr>

<tr bgcolor="lightblue"><td>3xx</td><td>1.00</td>
<td>1.00</td>
<td>0.46</td>
<td>0.79</td>
<td>0.85</td>
<td>0.82</td>
<td>0.89</td>
<td>0.67</td>
<td>0.89</td>
<td>0.79</td>
<td>0.81</td>
<td>0.80</td>
<td>0.63</td>
<td>0.76</td>
<td>0.90</td>
<td>0.71</td>
<td>0.95</td>
<td>0.37</td>
<td>0.87</td>
<td>0.83</td>
<td>0.69</td>
<td>0.80</td>
<td>0.94</td>
<td>0.67</td>
<td>0.67</td>
<td>0.79</td>
<td>0.92</td>
<td>0.26</td>
<td>0.94</td>
<td>0.68</td>
</tr>



<tr bgcolor="yellow"><td>H-mean</td><td>1.00</td>
<td>1.00</td>
<td>0.44</td>
<td>0.60</td>
<td>0.95</td>
<td>0.90</td>
<td>0.98</td>
<td>0.64</td>
<td>0.92</td>
<td>0.86</td>
<td>0.96</td>
<td>0.89</td>
<td>0.89</td>
<td>0.87</td>
<td>0.83</td>
<td>0.49</td>
<td>0.85</td>
<td>0.54</td>
<td>0.93</td>
<td>0.81</td>
<td>0.95</td>
<td>0.87</td>
<td>0.98</td>
<td>0.56</td>
<td>0.90</td>
<td>0.74</td>
<td>0.92</td>
<td>0.21</td>
<td>0.76</td>
<td>0.70</td>
</tr>

<tr bgcolor="lightblue"><td>Ext</td><td>1.00</td><td>1.00</td>
<td>0.59</td><td>0.80</td>
<td>0.97</td><td>0.92</td>
<td>0.99</td><td>0.64</td>
<td>0.96</td><td>0.89</td>
<td>0.97</td><td>0.90</td>
<td>0.93</td><td>0.90</td>
<td colspan='2'><center>Error</center></td>
<td colspan='2'><center>Error</center></td>
<td>0.96</td><td>0.84</td>
<td>0.96</td><td>0.87</td>
<td colspan='2'><center>Error</center></td>
<td>0.93</td><td>0.77</td>
<td colspan='2'><center>Error</center></td>
<td colspan='2'><center>Error</center></td>
</tr>

</tbody></table>


Table 1: Means of results obtained by participants on the benchmark test case
(corresponding to harmonic means). The full results are
available <a href="HTML/results.html">here</a>.</p>


<p>These results and those displayed in Figure&nbsp;1 single out a group of systems,
ASMOV, Lily, Falcon 0.7, OLA2, Prior+ and RiMOM which seem to perform these tests at the highest level of quality.
Of these, ASMOV, Lily and RiMOM seem to have slightly better results than the three others.</p>

<p align="center">
<img src="HTML/triangle.png"/><br />
Figure 1: Each point expresses the position of a system with regard to precision and recall.
</p>










<h2>Precision/recall graphs</h2>

<p>This year the apparently best algorithms provided their results
  with confidence measures. It is thus possible to draw
  precision/recall graphs in order to compare them.</p>
<p>
We provide in Figure&nbsp;2 the precision and recall graphs of this year.
They are only relevant for the results of participants who provided
confidence measures different of 1 or 0 (see the table in
the <a href="../index.html">Summary page</a>).
They also feature the results for edit distance on class names (edna) and the results of previous years (Falcon-2005 and RiMOM-2006). 
This graph has been drawn with only technical adaptation of the technique used in TREC.
Moreover, due to lack of time, these graphs have been computed by averaging the graphs of
each of the tests (instead to pure precision and recall).</p>

<p align="center">
<img src="HTML/prgraph.png"/><br />
Figure 2: Precision/recall graphs.
They cut the results given by the participants under a threshold necessary for achieving <i>n</i>% recall and
compute the corresponding precision. Systems for which these graphs are not meaningful
(because they did not provide graded confidence values) are drawn in dashed lines.
We remind the graphs for the best systems of the previous years, namely of Falcon in 2005 and RiMOM in 2006..
</p>

<h2>Comparison with previous years</h2>

<p>
Like the two previous years there is a gap between these systems and their followers.
The good news is that one system (OLA2) has achieved to fill this gap without significantly changing its
strategy (Disclosure: the author of these lines is a member of the
OLA2 team).</p>

<p>We have compared the results of this year's systems with the results
of the previous years on the basis of 2004 tests, see Table&nbsp;2.
The results of three best systems (ASMOV, Lily and RiMOM) are
comparable but never identical to the results provided in the previous
years by RiMOM (2006) and Falcon (2005).
Like Falcon last year, RiMOM provided this year lower results than last year. Figure&nbsp;2 shows that RiMOM has increased in precision and decreased in overall performance.
There seems to be a limit that systems are not able to overcome.
At the moment, it seems that these systems are at a level at which making more progress is very hard: we now have
strong arguments that having a 100% recall and precision on all these tests is not a reachable goal.
</p>

<p align="center">
<table border='2' frame='sides' rules='groups'>
<colgroup align='center' />
<colgroup align='center' span='4' />
<colgroup align='center' span='2' />
<colgroup align='center' span='2' />
<colgroup align='center' span='6' />
<thead valign='top'><tr><th>Year</th>
<th colspan='4'>2004</th>
<th colspan='2'>2005</th>
<th colspan='2'>2006</th>
<th colspan='6'>2007</th>
</tr></thead>
<tbody>
<tr>
<td>System</td>
<td colspan='2'>fujitsu</td>
<td colspan='2'>stanford</td>
<td colspan='2'>falcon</td>
<td colspan='2'>RiMOM</td>
<td colspan='2'>ASMOV</td>
<td colspan='2'>Lily</td>
<td colspan='2'>RiMOM</td>
</tr>
<tr><td>test</td>
<td>Prec.</td><td>Rec.</td><td>Prec.</td><td>Rec.</td><td>Prec.</td><td>Rec.</td><td>Prec.</td><td>Rec.</td><td>Prec.</td><td>Rec.</td><td>Prec.</td><td>Rec.</td><td>Prec.</td><td>Rec.</td></tr>
<tr bgcolor="lightblue"><td>1xx</td><td>0.99</td><td>1.00</td><td>0.99</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td><td>1.00</td></tr>
<tr><td>2xx</td><td>0.93</td><td>0.84</td><td>0.98</td><td>0.72</td><td>0.98</td><td>0.97</td><td>1.00</td><td> 0.98</td><td>0.99</td><td>0.99</td><td>1.00</td><td>0.98</td><td>1.00</td><td>0.97</td></tr>
<tr bgcolor="lightblue"><td>3xx</td><td>0.60</td><td>0.72</td><td>0.93</td><td>0.74</td><td>0.93</td><td>0.83</td><td>0.83</td><td>0.82</td><td>0.85</td><td>0.82</td><td>0.81</td><td>0.80</td><td>0.69</td><td>0.80</td></tr>
<tr bgcolor="yellow"><td>H-means</td><td>0.88</td><td>0.85</td><td>0.98</td><td>0.77</td><td>0.97</td><td>0.96</td><td> 0.97</td><td> 0.96</td><td>0.97</td><td>0.97</td><td>0.97</td><td>0.96</td><td>0.95</td><td>0.95</td></tr>
</table>
Table 2: Evolution of the best scores over the years (on the basis of 2004 tests).
</p>

<h2>References</h2>

<p>[Ehrig&nbsp;2005] Marc Ehrig and Jérôme Euzenat. Relaxed precision and recall for ontology matching. In Ben Ashpole, Jérôme Euzenat, Marc Ehrig, and Heiner Stuckenschmidt, editors, Proc. K-Cap 2005 workshop on Integrating ontology, Banff (CA), pages 25-32, 2005.</p>

<hr />
<small>
<center>http://oaei.ontologymatching.org/2007/results/benchmarks/</center>
<hr />
$Id: index.html,v 1.1 2007/10/17 14:03:54 euzenat Exp euzenat $
</small>

</body></html>
