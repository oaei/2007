<html>
<head>
<style type="text/css">
<!--
a:link {  color: #0074CA; text-decoration: none}
a[href]:hover {  color: #0074CA; text-decoration: none; background-color: #D3DBFE}
a:visited {  color: #0084E6; text-decoration: none}
.justification { font-family: Arial, Helvetica, sans-serif; font-size: 7pt; letter-spacing: normal; text-align: justify; word-spacing: normal ; font-style: normal; font-weight: normal; font-variant: normal; white-space: normal}
-->
</style>
<title>Ontology Alignment Evaluation 2007::results</title>
</head>
<body>

<a href=".."><img width="20%" border="0" src="../../oaei.jpg" align="right" /></a>

<h1>Ontology Alignment Evaluation Initiative</h1>
<h1>2007 Results</h1>

<p>Here are the official <!--and definitive--> results of the Ontology
 Alignment Evaluation 2007. They will be presented in Busan (KR) at
 the <a href="http://om2007.ontologymatching.org">ISWC 2007 Ontology matching workshop</a>.</p>
<p>A <a href="oaei2007.pdf">synthesis paper</a> in the proceedings of
  this workshop summarises the main results of the 2007 campaign (the
  present version has been lightly updated with regard to the
  proceedings version).
  Here further data and update in the results are
  available. This page is the official result of the evaluation.</p>
<p>The papers provided by the participants have been collected in the 
<a href="../../doc/Proceedings-OM-2007.pdf">ISWC 2007 Ontology matching
  workshop proceedings (PDF)</a><!-- which are also published as
<a href="http://ceur.or-wsg/Vol-225/">CEUR Workshop Proceedings volume 225</a>-->.</p>

<h2>General summary</h2>

<p>This year again, we had more participants than in previous years: 4 in 2004, 7 in 2005, 10 in 2006, and 17 in 2007.
We can also observe a common trend: participants who
keep on developing their systems improve the evaluation results over
  the years.
</p>
<!--p>
%Unlike last year, we do not note an increase on tool compliance:
%a number of participants have returned results that cannot be automatically exploited by a
%computer without editing them. Last year increase in compliance was rather turned toward
%our own ontologies that the tools refused to use.
%However, the tools which have participated to the evaluation have at least be
%confronted to the problems of encoding, URI correctness, XML conformance and format.
%Some of them have improved during the process. So in general, the systems who participated should be more usable.
</p-->
<p>
We have had not enough time so far to validate the results which had been provided by the participants,
but we scrutinized some of the results leading to improvements for some participants and retraction from others.
Validating these results has proved feasible in the previous years so
we plan to do it again in the future.
</p>
<p>
We summarize the list of participants in the table below.
Similar to last year not all participants provided results for all tests.
They usually did those which are easier to run, such as benchmark, directory and conference.
The variety of tests and the short time given to provide results have certainly
prevented participants from considering more tests.
</p>
<p>
There are two groups of systems: those which can deal with large taxonomies
(food, environment, library) and those which cannot.
The two new test cases (environment and library) are those with the least number of participants.
This can be explained by the size of ontologies or their novelty - there are no past results to compare with.
</p>

<p align="center">
<table border="2" frame='sides' cellspacing="80" rules='groups'>
<thead valign='top'>
<tr bgcolor="lightblue">
<td>Software</td><td>confidence</td><td>benchmark</td><td>anatomy</td><td>directory</td><td>food</td><td>environment</td><td>library</td><td>conference</td><td>&Sigma;</td></tr></thead>

<tr><td>AgreementMaker </td><td> &bull;  </td><td>  </td><td> &bull; </td><td>  </td><td>  </td><td>  </td><td>  </td><td> </td><td>1</td></tr>
<tr><td>AOAS </td><td> &bull; </td><td> </td><td> &bull; </td><td> </td><td>  </td><td> </td><td>  </td><td></td><td>1</td></tr>
<tr><td>ASMOV </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td>  </td><td>  </td><td>  </td><td> &bull;</td><td>4</td></tr>
<tr><td>DSSim </td><td> &bull; </td><td> &bull; </td><td> &bull;  </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td> </td><td>6</td></tr>
<tr><td>Falcon-AO v0.7 </td><td> &bull;  </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td>7</td></tr>
<tr><td>Lily </td><td>  </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td>  </td><td>  </td><td>  </td><td> &bull; </td><td>4</td></tr>
<tr><td>OLA2 </td><td> &bull; </td><td> &bull; </td><td>  </td><td> &bull; </td><td>  </td><td>  </td><td>  </td><td> &bull;</td><td>3</td></tr>
<tr><td>OntoDNA </td><td>  </td><td> &bull; </td><td> </td><td> &bull; </td><td>  </td><td>  </td><td>  </td><td> &bull;</td><td>3</td></tr>
<tr><td>OWL-CM </td><td> </td><td> &bull; </td><td> </td><td> </td><td>  </td><td>  </td><td>  </td><td></td><td>1</td></tr>
<tr><td>Prior+ </td><td> &bull;  </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td> </td><td>  </td><td> </td><td>4</td></tr>
<tr><td>RiMOM </td><td> &bull;  </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td>  </td><td>  </td><td> </td><td>4</td></tr>
<tr><td>SAMBO </td><td>  </td><td> &bull; </td><td> &bull; </td><td>  </td><td>  </td><td>  </td><td>  </td><td> </td><td>2</td></tr>
<tr><td>SCARLET </td><td> ? </td><td> </td><td> </td><td> </td><td>  &bull; </td><td> </td><td> </td><td></td><td>1</td></tr>
<tr><td>SEMA </td><td>  </td><td> &bull; </td><td>  </td><td>  </td><td>  </td><td>  </td><td>  </td><td> &bull;</td><td>2</td></tr>
<tr><td>Silas </td><td> ? </td><td> </td><td> </td><td> </td><td>  </td><td> </td><td> &bull; </td><td></td><td>1</td></tr>
<!--tr><td>SODA </td><td>  </td><td> &bull; </td><td>  </td><td> &bull; </td><td> </td><td> </td><td> </td><td> </td><td></td></tr-->
<tr><td>TaxoMap </td><td> &bull;   </td><td> &bull; </td><td>  &bull; </td><td> </td><td> </td><td>  </td><td>  </td><td> </td><td>2</td></tr>
<tr><td>X-SOM </td><td> &bull;  </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td> &bull; </td><td> </td><td> </td><td></td><td>4</td></tr>
<tr><td>
<tr><td>&Sigma;=17 </td><td> 10 </td><td> 13 </td><td> 11 </td><td> 9 </td><td> 6 </td><td> 2 </td><td> 3 </td><td> 6</td><td></td></tr>
</tbody>
<caption>Table: Participants and the state of their submissions.
Confidence stands for the type of result returned by a system:
it is ticked when the confidence has been measured as a non boolean value.</caption></table>
<br />
</p>

<p>This year we have been able to devote more time to performing these tests and evaluation (three full months).
This is certainly still too little especially during the summer period allocated for that.
However, it seems that we have avoided the rush of previous years.
</p>

<h2>Track by track results</h2>

<p>The summary of the results track by track will be provided in the
  following six sections as soon as they are made available:
<ul compact="1">
<li><a href="benchmarks/"><b>benchmarks</b></a></li>
<li><a href="http://webrum.uni-mannheim.de/math/lski/align2007/results.html"><b>anatomy</b></a></li>
<li><a href="directory/"></a><b>directory</b></li>
<li><a href="food/"></a><b>food</b></li>
<li><a href="environment/"></a><b>environment</b></li>
<li><a href="http://www.few.vu.nl/~aisaac/oaei2007/results.html"><b>library</b></a></li>
<li><a href="http://nb.vse.cz/~svabo/oaei2007/"><b>conference</b></a></li>
</ul>

</p>

<h2>Lesson learnt</h2>

<p>The most important applied lesson learned from last year is that we have been able to revise the schedule so we had more time for evaluation.
But there remain lessons not really taken into account that we identify with an asterisk (*).
So we reiterate those lessons that still apply with new ones,
  including:
<dl>
<dt>A)</dt><dd> This is a trend that there are now more matching systems and more systems are
able to enter such an evaluation. This is very encouraging for the progress of the field.</dd>
<dt>B)</dt><dd> Many systems have been entering the campaign for several years.
This means that we are not dealing with a continuous flow of prototypes but with systems
on which there is
a persistent development. These systems tend to improve over years.</dd>
<dt>C*)</dt><dd> The benchmark test case is not discriminant enough between systems.
It is still useful for evaluating the strength and weakness of algorithms but does not seem
to be sufficient anymore for comparing algorithms. We will have to look into better alternatives.</dd>
<dt>D)</dt><dd> We have had more proposals for test cases this year (we had actively looked for them). However, the difficult lesson is that proposing
a test case is not enough, there is a lot of remaining work in preparing the evaluation. Fortunately, with tool improvements, it will be easier
to perform the evaluation.
We would also like to have more test cases for expressive ontologies.</dd>
<dt>E*)</dt><dd> It would be interesting and certainly more realistic,
to provide some random gradual degradation of the benchmark tests
(5% 10% 20% 40% 60% 100% random change) instead of a general discarding of features one by one.
This has still not been done this year but we are considering it seriously for the next year.</dd>
<dt>F)</dt><dd> We have detected this year, through some random verifications, some submissions which were not strictly complying
to the evaluation rules. We may have to be more strict about control in future.</dd>
<dt>G)</dt><dd> Contrary to what has been noted in 2006, a significant number of systems were unable to output
syntactically correct results (i.e., automatically usable by another program). Since fixing these mistakes by hand
is becoming too much work, we plan to go towards automatic evaluation in which participants have to input correct results.</dd>
<dt>H)</dt><dd> There seems to be partitions of the systems: between systems able to deal with large test sets and
systems unable to do it, between system robust on all tracks and those which are specialized (see Table).
These observations remain to be further analyzed.</dd>
</dl>
</p>

<h2>Future plans</h2>

<p>Future plans for the Ontology Alignment Evaluation Initiative are certainly to go ahead and to improve the functioning of the evaluation campaign. This involves:
<ul compact="1">
<li> Finding new real world test cases, especially expressive ontologies;</li>
<li> Improving the tests along the lesson learned;</li>
<li> Accepting continuous submissions (through validation of the results);</li>
<li> Improving the measures to go beyond precision and recall (we have done this for generalized precision and recall as well as for using precision/recall graphs, and will continue with other measures);</li>
<li> Developing a definition of test hardness.</li>
</ul>
Of course, these are only suggestions that will be refined during the coming year.</p>

<h2>Conclusions</h2>

<p>
This year we had more systems that entered the evaluation campaign
as well as more systems managed to produce better quality results compared to the previous years.
Each individual test case had more participants than ever.
This shows that, as expected, the field of ontology matching is getting stronger (and we hope that evaluation has been contributing to this progress).
</p>
<p>
On the side of participants, it seems that there is clearly a problem of size of input that should be addressed in a general way.
We would like to see more participation on the large test cases. On the side of organizers,
each year the evaluating of matching systems becomes more
complex.</p>

<p>
Most of the participants have provided description of their systems and their experience in
the evaluation (but SCARLET which system is described in a regular
ISWC 2007 paper).
These OAEI papers, like the present one, have not been peer reviewed.
Reading the papers of the participants should help people involved in ontology matching to find what makes these algorithms work and what could be improved.
</p>
<p>
The Ontology Alignment Evaluation Initiative will continue these tests by improving both test cases and testing methodology for being more accurate.
Further information can be found
on <a href="http://oaei.ontologymatching.org">this site</a>.
</p>
<hr />
<small>
<center>http://oaei.ontologymatching.org/2007/results/</center>
<hr />
$Id: index.html,v 1.4 2007/10/17 14:02:23 euzenat Exp euzenat $
</small>

</tbody></table>
</body></html>
