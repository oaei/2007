<html>
<head>
<title>Ontology Alignment Evaluation Initiative::2005</title>
</head>
<body bgcolor="white" face="tahoma, arial">

<table width="100%">
<tr BGCOLOR="yellow" ALIGN="center"><td>
The 2005 results are available <a href="results">here</a>.
</td></tr>
</table>

<a href=".."><img width="20%" border="0" src="../oaei.jpg" align="right" /></a>

<h1>Ontology Alignment Evaluation Initiative</h1>
<h1>2005 Campaign</h1>

<p>The increasing number of methods available for schema matching/ontology integration 
  suggests the need to establish a consensus for evaluation of these methods. 
  There is now a coordinated international initiative to forge this consensus. 
  After the two events organized in 2004 (namely, the <a
  href="http://www.atl.external.lmco.com/projects/ontology/i3con.html">Information 
  Interpretation and Integration Conference (I3CON)</a> and the <a
  href="../2004/Contest/">EON Ontology Alignment Contest</a>), this year we organize 
  one unique evaluation of which the outcome will be presented at the <a href="http://km.aifb.uni-karlsruhe.de/ws/intont2005">Workshop 
  on Integrating Ontologies</a> held in conjunction with K-CAP 2005 at Banff (Canada) 
  on October 2, 2005.</p>

<h2>Alignment problems</h2>

<p>This year's campaign will consist of three parts: it will feature two real world blind tests (anatomy and directory) and a systematic benchmark test
  suite. By blind tests it is meant the result expected from the test
  is not known in advance. The evaluation organisers 
  provide the participants with the pairs of 
  ontologies to align as well as (in the case of the systematic 
  benchmark suite only) expected results. The ontologies are described in OWL-DL and serialized 
  in the RDF/XML format. The expected alignments are provided in a standard format 
  expressed in RDF/XML and described in <a
  href="http://co4.inrialpes.fr/align/">http://co4.inrialpes.fr/align/</a>.

<p>The <b>anatomy</b> real world case covers the domain 
  of body anatomy and will consists of two ontologies with an approximate size 
  of several 10k classes and several dozen of relations.</p>

<p>The <b>directory</b> real world case consists of alignming web
  sites directory (like open directory or Yahoo's). It is more than
  two thousand elementary tests.</p>

<p>Like for last year's EON contest, a <b>systematic benchmark series</b> has 
  been produced. The goal of this benchmark series is to identify the areas in 
  which each alignment algorithm is strong and weak. The test is based on one 
  particular ontology dedicated to the very narrow domain of
  bibliography and a 
  number of alternative ontologies of the same domain for which alignments are 
  provided.</p>

<h2>Evaluation process</h2>

<p>The evaluation will be processed in three successive steps.</p>

<h3>Preparatory Phase </h3>

<p>The ontologies and alignments of the evaluation are provided in advance during 
  the period between June 1st and July 1st. This gives potential participants the 
  occasion to send observations, bug corrections, remarks and other test cases 
  to the organizers. The goal of this primary period is to be sure that the delivered 
  tests make sense to the participants. The feedback is important, so all participants 
  should not hesitate to provide it. The tests will certainly change after this 
  period, but only for ensuring a better participation to the tests. The final 
  test base will be released on July 4th. </p>

<h3>Execution Phase </h3>

<p>During the execution phase the <b>participants</b> will use their
  algorithms to automatically align the ontologies of both part. The  
  participants should only use one algorithm and the same set of
  parameters for all tests. Of course, it is regular to select the
  set of parameters that provide the best results. Beside the
  parameters the input of the algorithms must be the two provided
  ontology to align and any general purpose resource available to
  everyone (that is no resourse especially designed for the test). In
  particular, the participants should not use the data (ontologies
  and results) from other test sets to help their algorithm. And
  cheating is not fair...</p>
<!-- For 
  some of the tests the organizers further provide sets of alignments which are 
  considered to be preknown to the system. These can be given to the alignment 
  algorithm prior to running it. They are based on a draw of 10% of
  the target alignment. -->
<p>The <b>participants</b> will provide their alignment for each test
  in the <a href="http://co4.inrialpes.fr/align">Alignment
  format</a>. The results will be provided in a zip file containing
  one directory per test (named after its number) and each directory
  containing one result file in the
  RDF/XML <a href="http://co4.inrialpes.fr/align">Alignment 
  format</a> with always the same name
  (e.g., participant.rdf). This should yield the following structure:
<pre>
participant.zip
+- benchmark
|  +- 101
|  |  +- participant.rdf
|  +- 102
|  |  +- participant.rdf
|  + ...
+- anatomy
|  +- participant.rdf
+- directory
   +- 1
   |  +- participant.rdf
   + ...
</pre>
</p>
<p>They will also provide a paper to be published in the
  proceedings and a link to their program and parameter set.</a>
<p>The only interesting alignments are those involving classes and
  properties of the given ontologies. So the alignments should not
  align individuals, nor entities from the external ontologies.</p>

<!--table width="75%" border="1">
  <tr> 
    <th>Ontology 1</th>
    <th>Ontology 2</th>
    <th>Input Alignments</th>
    <th>Expected Result Alignments</th>
	<th>Comments</th>
  </tr>
  <tr> 
    <td>onto101.owl</td>
    <td>onto102.owl</td>
    <td>-</td>
    <td>onto101_102.xml</td>
	<td>&nbsp; </td>
  </tr>
  <tr> 
    <td>...</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
    <td>&nbsp;</td>
		<td>&nbsp; </td>
  </tr>
</table-->

<h3>Evaluation Phase</h3>

<p>The <b>organizers</b> will evaluate the results of the algorithms
  used by the participants and provide comparisons on the basis of the
  provided alignments.</p>
<p>In order to ensure that it will be possible to process
  automatically the provided results, the <b>participants</b>
  are requested to provide (preliminary) results by August 15th. In
  the case of the 
  real world ontologies only the organizers will do the evaluation
  with regard to the withheld alignments. An email with the location
  of the required zip file must be sent to the contact addresses below.
</p>
<p>The standard evaluation measures will be precision and recall
  computed against the reference alignments. For the matter of
  aggregation of the measures we will use weighted harmonic means
  (weight being the size of reference alignment). Another improvement
  that might be used is the computation of
  <a href="http://www.oracle.com/technology/products/text/htdocs/imt_quality.htm?_template=/ocom/ocom_item_templates/print">precision/recall graphs</a>.</p>
<p>Further, it is planned to
  introduce new measures addressing some limitations of precision and
  recall. These will be presented at the workshop discussion
  in order for the participants to provide feedback on the
  opportunity to use them in a further evaluation.</p>
<p>Finally, in an experimental way, we will attempt this year at
  reproducing your results. To that extent, we require a link to the
  tool and the set of parameters.</p>

<h2>Schedule Overview</h2>

<p> First publication of test cases: June 10th, 2005.<br />
  Comments due: anytime before July 1st, 2005<br />
  Final publication of test cases: July 4th, 2005.<br />
  Preliminary results due: August 15th, 2005.<br />
  Camera ready copies: September 2nd, 2005.<br />
  Workshop : October 2nd, 2005.<br />
</p>

<h2>Presentation</h2>

<p>From the results of the experiments the participants are expected
  to provide the organisers with a paper to be published in the proceedings   
  of the workshop. The paper must be at most 8 pages long and formatted according 
  to the guidelines of the main conference 
  (<a href="www.kcap05.org">http://www.kcap05.org/</a>). 
  To ensure easy comparability among the participants it has to follow the given 
  outline. A package with LaTeX and Word templates can be found <a href="templates.zip">here</a>

The above mentionned paper must be sent by September 2nd to
  Heiner Stuckenschmidt (heiner (à) cs . vu . nl) with copy to
  Jerome . Euzenat (à) inrialpes . fr.</p>
<p>
<ul 1>
  <li> 1) Presentation of the system<br />
    <ul 1>
      <li>1.1) State, purpose, general statement<br/>
      It is interesting to see here what was the purpose of the
      algorithm used in terms of the following categories: ontology
      matching, schema matching, version matching, directory
      matching. The purpose of this information is to study the
      correlation between these purposes and the success in some
      particular tests.</li>
      <li>1.2) Specific techniques used</li>
      <li>1.3) Adaptations made for the evaluation</li>
    </ul>
  </li>
  <li>2) Results<br />
    2.x) a comment for each test</li>
  <li>3) General comments<br />
    <ul 1>
      <li>3.1) Comments on the results (strength and weaknesses)</li>
      <li>3.2) Discussions on the way to improve the proposed system</li>
      <li>3.3) Comments on the test cases</li>
      <li>3.4) Comments on measures</li>
      <li>3.5) Proposed new measures</li>
    </ul>
  </li>
  <li>4) Raw results<br />
    <ul 1>
      <li>4.1) Links to the set of provided alignments (in align format)</li>
      <li>4.2) Matrix format<br />
      If possible provide the run time values in hh.mn.ss.mms format.</li>
    </ul>
  </li>
</ul>
</p>

<p>The results from both, the participants and the organizers, will be presented 
  at the <a href="http://km.aifb.uni-karlsruhe.de/ws/intont2005"> Workshop on 
  Integrating Ontologies</a> at K-CAP 2005 taking place at Banff (Canada) on October, 
  2nd 2005. We hope to see you there.</p>

<h2>Tools and material</h2>

<h3>Systematic benchmarks</h3>

<p>The zip file containing all the data is 
<a href="benchmarks/bench.zip">here</a> as well as the <a href="benchmarks/">full description of the benchmarks</a>.</p>

<h3>Real world case: Directory</h3>

<p>Real world case on directories can be found <a href="directory">here</a>.</p>

<h3>Real world case: Anatomy</h3>

<p>Real world case on anatomy can be found <a href="anatomy">here</a>.</p>

<!--h3>Versions</h3>

<p>Several version of the tests could provided as soon as the
  participants found problems with the tests. The tests displayed
  here are the last version.</p>

<p>The <a href="log10-11.txt">release notes</a> about these versions are available.</p>

<p>Further versions might be made available in the future.</p-->

<h3>Processing tools</h3>

<p>The <b>participants</b> may use the <a
  href="align.html">Alignment API</a> for generating and manipulating their alignments (in particular for computing 
  evaluation of results). </p>

<!--p>A Ant <a href="build.xml">build.xml</a> file is also provided that
  can be used for generating the HTML pages. It will be enhanced for
  computing the characteristics of the results.</p-->

<!--<h2>Results</h2>
<p>The fully detailled results, as provided from the competitors will be published 
  here.</p>-->

<h2>Steering Committee</h2>

<p>Benjamin Ashpole (Lockheed Martin Advanced Technology Lab.)<br>
  Marc Ehrig (University of Karlsruhe)<br>
  J&eacute;r&ocirc;me Euzenat (INRIA Rh&ocirc;ne-Alpes)<br>
  Lewis Hart (Applied Minds)<br>
  Todd Hughes (Lockheed Martin Advanced Technology Labs)<br>
  Natasha Noy (Stanford University)<br>
  Heiner Stuckenschmidt (Vrije Universiteit Amsterdam)<br>
  Petko Valtchev (Universit&eacute; de Montr&eacute;al, DIRO)</p>

<!--h1>Acknowledgements</h1>

<p>Many resources have been used for setting up this test (I must put links):
<ul compact="1">
<li>BibTeX</li>
<li>UMBC BibTeX</li>
<li>MIT BibTex</li>
<li>Java BibTeX parser</li>
<li><a href="http://www.aifb.uni-karlsruhe.de/ontology">Karlsruhe
 ontology</a>
 and bibtex to RDF translator that we adapted</li>
<li>INRIA's adaptation of the ontologies and the translator above by
  Antoine Zimmermann.</li>
<li><a href="http://www.atl.external.lmco.com/projects/ontology/">Lockheed evaluation format</a>.</li>
<li>WonderWeb/Manchester's <a href="http://phoebus.cs.man.ac.uk:9999/OWL/Validator">OWL species validator</a></li>
</ul>
</p>

<p>Various people helped testing or suggested improvements and tests:
<ul compact="1">
<li>York Sure and Oscar Corcho for welcoming this at their EON workshop;</li>
<li>Heiner Stuckenschmidt helped providing several sorts of tests;</li>
<li>Liz Palmer proposed variation tests;</li>
<li>Natasha Noy and David Loup have been very helpful in noticing and
  correcting bugs.</li>
</ul>
</p-->

<h2>Organizers</h2>

<p><ul compact="1">
<li>J&eacute;r&ocirc;me Euzenat (INRIA Rh&ocirc;ne-Alpes)</li>
<li>Heiner Stuckenschmidt (VU Amsterdam)</li>
<li>Mikalai Yatskevich (University of Trento)</li>
</ul></p>

<hr />
<center><small>http://oaei.ontologymatching.org/2005</small></center>
<hr />
$Id: index.html,v 1.20 2006/01/11 17:21:11 euzenat Exp $
</body>
</html>

